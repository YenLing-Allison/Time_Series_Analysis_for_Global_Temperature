---
title: "Problem 3 (15 credits)"
subtitle: "HW3"
author: "Norah Lu (lu000566), Weijing Luo (luoxx644), Xingyue Wang (wan01786), Allison Liu (liu02637)"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: pdf_document
---

```{r}
suppressPackageStartupMessages({
  library(TSA)
  library(ggplot2)
  library(dplyr)
  library(forecast)
  library(tseries) #Only for the ADF test for testing stationarity
})

```

# Time Series Data of Your Choice

## Background

This homework problem will allow you to apply the learned time series analysis and forecasting skills to your own or favorite dataset. This dataset could be your own data (from your interested hobby groups, sports or video game records, previous jobs, past school works, etc). Notice that, we DON'T require data disclosure so please feel free to use your own data if you would like us to help you understand the result. Or if you don't have any time series dataset, please feel free to get one from the Internet. Any topics are welcome! 

Hint: If you have trouble finding a good dataset, sources of public time series data include `kaggle.com` where many of the class examples came from, and `Yahoo Finance` which provides rich information about historical prices of nearly every US stock. Nevertheless, data from sources other than `Kaggle` and `Yahoo Finance` are also encouraged. For example, if you are a sport fun or a video game fun, you may analyze a series of data downloaded from the corresponding website.

So, please feel free to explore!

## Importance

Notice that, the questions we provided in this problem are mostly real-world tasks that we encountered in many data scientists' daily job, including those working in the financial or retail sector (such as hedge fund companies). So while finishing the questions as required assignments, please make sure to take a few minutes to understand why these questions are raised, and what's the standard procedures to address them. 

The credit will be given based on whether you do everything following the standard procedures we learned in our class, as opposed to the results such as whether the forecasting accuracy is good.

## General Requirements

However, we do have some very general and mild requirements in order for the analysis to be valid.

1. Please make sure the time series contains at least $T=500$ time points. The final credits will be prorated if $T$ is less than 500 (`floor` to the nearest hundred, for example, 499 will be counted as 400, and hence $400/500=80\%$ credits will be given).

2. Please make sure the data are REAL data, not simulated ones. Given there're plenty of available datasets online, there's no motivation to use simulated data. Only $50\%$ credits will be given if we find out the data are simulated.

3. Also make sure the data are non-trivial (having sufficient data variation and possibly a trend). For example, it is trivial to analyze a series of 500 zeros, denoting something like "the number of spacecrafts I owned in the past 1.5 years". 0 credits will be given if the data are regarded as trivial.

4. If two groups happen to use the same dataset (or one dataset being the subset of another), which rarely happens, we reserve the right to place the two homework under scrutiny.

5. Please do not use any datasets (or their subsets) used in the lectures or previous homework. Otherwise, 0 credits will be given to this problem.

## Question 1 (1 credit)

Please briefly describe the background of your dataset as I did for the Boston Crime Data in Homework 1 Problem 3, and its source (link) if you are using public data.

#### Data source: https://datahub.io/core/global-temp?ref=hackernoon.com#readme
```{r}
#Enter your description here
# Data are included from the GISS Surface Temperature (GISTEMP) analysis and the global component of Climate at a Glance (GCAG). Two datasets are provided: 1) global monthly mean and 2) annual mean temperature anomalies in degrees Celsius from 1880 to the present. 
#GISTEMP combined Land-Surface Air and Sea-Surface Water Temperature Anomalies [i.e. deviations from the corresponding 1951-1980 means]. Global-mean monthly [â€¦] and annual means, 1880-present, updated through most recent month.
#GCAG is global temperature anomaly data come from the Global Historical Climatology Network-Monthly (GHCN-M) data set and International Comprehensive Ocean-Atmosphere Data Set (ICOADS), which have data from 1880 to the present. These two datasets are blended into a single product to produce the combined global land and ocean temperature anomalies. The available timeseries of global-scale temperature anomalies are calculated with respect to the 20th century average.
#For this question, we are working on the monthly mean temperature anomalies in degree Celsius relative to a base period from 1880 to 2016 with GCAG data. 

data = read.csv('monthly_csv.csv')
data = data %>% filter(Source=='GCAG')

```

## Question 2 (1 credit)

Please plot your data and provide the sample size. We will use the last 10 data points for testing, while the rest of the series for training.

```{r}
#Please provide your code here
plot(data[1644:1,3],type="o",ylab="Mean",main="Monthly mean temperature anomalies in degree Celsius relative to a base period from 1880 to 2016")

```

#### Sample size = 1644
```{r}
T = nrow(data)
T
```

## Question 3 (2 credits)

On the TRAINING set:

Please (make transformations if necessary, and) use the `ADF test` to check for stationarity. Remove trend if necessary, and check the residuals for spurious regression (proof of random walk)

Check ACF, PACF, and EACF for the order of the ARMA model (after differencing, if it has a random walk). Use AIC or BIC to select a final model from your candidate models. Report the orders.

```{r}
#Please provide your code here
df=ts(data[1644:1,3],start=1,end=1644)
df_train=ts(df[1:1634],start=1,end=1634)
df_test=ts(df[1635:1644],start=1635,end=1644)

adf.test(df_train) #since the p value is 0.01, which is significantly small, we conclude that the data is stationary

#Final Model: ARIMA(4,0,0)

```

####since the p value is 0.01, which is significantly small, we conclude that the data is stationary
```{r}
Acf(df_train)
```
```{r}
Pacf(df_train)
```

Based on the Pacf plot, the model may be AR(4)
```{r}
#eacf plot
eacf(df_train)
```
```{r}
auto.arima(df_train,stationary = TRUE, seasonal = FALSE, ic='aic')
```
## Question 4 (2 credits)

Fit your final model, write down the model (You may write down only the non-seasonal part, if you model contains seasonality).

Report the significance of the model coefficients.

Hints: 

 - Check Homework 2 - Problem 3 - Question 1(b) and 1(c) for how to write a model and how to define significance.

Answer:

$$Y_t= 0.56\cdot Y_{t-1} + 0.19 \cdot Y_{t-2} + 0.11 \cdot Y_{t-3} + 0.12 \cdot Y_{t-4} + e_t$$

```{r}
#arima_fit
arima_fit  = Arima(df_train,c(4,0,0))
arima_fit
```



## Question 5 (3 credits)

Forecast on the testing set. Provide RMSE. 

Plot the fitted value, as well as $80\%$ and $95\%$ prediction intervals, superimposed on the raw data.

Explain whether your selected model fit the data well.

Hint: 

 - Please check the code of Lecture 7, where similar things are done for the US Consumption data, CO2 data and Bitcoin data

 - If you made transformations on your training data, please use the same transformation on your testing data as well
 
```{r}
arima_pred <- forecast(arima_fit,h=10)
autoplot(arima_pred)+
  autolayer(df_test, series="Data") +
  autolayer(arima_pred$mean, series="Forecasts") +
  labs(title="Monthly mean temperature anomalies in degree Celsius relative to a base year", y="Mean")
```
```{r}
library(Metrics)
rmse(arima_pred$mean,df_test)
```

## Question 6 (6 credits)

Please do the same forecasting task in Question 5, with either XGBoost or LSTM. 

Report the RMSE of the selected method.

For the selected method, plot the fitted value, superimposed on the raw data (prediction intervals are not required).

Comments on the performance of XGBoost or LSTM compared with ARIMA, in terms of both accuracy and computational speed. Which one is better for your data? 

Hint: 

1. Check the last couple of slides in Lecture 7 in order to determine the input for these models

2. Please feel free to use Python or other software to run XGBoost or LSTM if your code (e.g., from previous classes) was ready. But please paste all code as comments in the area below for reproducibility reason.

3. For LSTM, it's OK if you only have time to try a couple of layers with a few neurons.

4. Some of my experiences are: ARIMA has the advantages of being fast, and being able to provide prediction intervals as a statistical model. But XGBoost and LSTM may provide better forecasting accuracy.


```{r}
#Please provide your code, explanation and figures here, regardless of what software you use
#Please provide your code, explanation and figures here, regardless of what software you use

# # Use the monthly global temperature time series data
# file_path = '/Users/user/Downloads/monthly_csv.csv'
# # Load the CSV file into a DataFrame
# df = pd.read_csv(file_path)
# 
# # filter the Source = 'GCAG'
# df = df[df['Source'] == 'GCAG']
# 
# # Reverse data into chronological order
# df = df.sort_values(by='Date', ascending=True)
# 
# data = df['Mean'].values
# 
# # LSTM model
# def create_time_series_dataset(data, look_back):
#     X, y = [], []
#     for i in range(len(data) - look_back):
#         X.append(data[i:i+look_back])
#         y.append(data[i+look_back])
#     return np.array(X), np.array(y)
#     
# look_back = 4  # Number of past time steps to consider: decide by the order of AR model in ARIMA
# 
# X, y = create_time_series_dataset(data, look_back)
# 
# look_forward = 10  # Number of future time steps to predict (i.e., the horizon)
# split_index = int(len(X) - look_forward) #The rest of the dataset is considered the training data
# X_train, X_test = X[:split_index], X[split_index:]
# y_train, y_test = y[:split_index], y[split_index:]
# 
# X_train = X_train.reshape(-1, look_back, 1) #(batch_size, time_steps, input_features)
# X_test = X_test.reshape(-1, look_back, 1)
# 
# # LSTM model
# if 'model' in locals():
#     del model
# 
# # We then set a random seed for reproducibility.
# tf.random.set_seed(66)
# 
# # The LSTM layer provides memory to the network and allows it to learn long-term dependencies.
# model = Sequential([
#     LSTM(64, activation='relu', input_shape=(look_back, 1)),
#     Dense(64, activation='relu', input_shape=(look_back, 1)),
#     Dense(64, activation='relu', input_shape=(look_back, 1)),
#     Dense(64, activation='relu', input_shape=(look_back, 1)),
#     Dense(64, activation='relu', input_shape=(look_back, 1)),
#     Dense(64, activation='relu', input_shape=(look_back, 1)),
#     Dense(64, activation='relu', input_shape=(look_back, 1)),
#     Dense(64, activation='relu', input_shape=(look_back, 1)),
#     Dense(1)
# ])
# 
# model.compile(optimizer='adam', loss='mse') # Adam optimizer and MSE loss function for regression
# 
# model.fit(X_train, y_train, epochs=32, batch_size=64)
# 
# y_pred = model.predict(X_test)
# rmse = np.sqrt(np.mean((y_pred - y_test)**2))
# print(rmse) 
```
#### RMSE = 0.2069
